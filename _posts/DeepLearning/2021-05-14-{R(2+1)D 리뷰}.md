---
layout: single
title: "A Closer Look at Sptiotemporal Convolutions for Action Recognition 리뷰"
categories:
  - DeepLearning
classes: wide
tags:
  - jeykll
  - github
  - Minimalmistakes
  - blog
  - DeepLearning
use_math: true
comments: true
---

###### A Closer Look at Sptiotemporal Convolutions for Action Recognition 리뷰  

논문 내용 정리하였습니다. 다른 논문 리뷰와는 다르게 내용이 재미있어서 논문 그대로의 내용을 가져왔습니다.   

#### 논문 다운 링크  
https://arxiv.org/pdf/1711.11248v3.pdf    

## Abstract  

이 논문에서는 비디오 분석을 해서한 여러 형태의 spatiotemporal convolution에 대해 논의 및 action recognition에 미치는 영향을 연구한다. Motivation은 비디오의 각 frame에 적용된 2D CNN이 action recognition에서 solid 성능을 유지하고 있다는 관찰에서 시작된다. 우리는 2D CNN보다 3D CNN이 정확도가 더 좋은 걸 보여주고, 3D Conv Filter를 spatial과 temporal components yield로 분리하면 정확도가 크게 향상되는 것을 보여준다. 사용한 Dataset은 Sport-1M, UCF-101, Kinetics, HMDB51를 사용하였고 새로운 spatiotemporal block R(2+1)D 인 produces CNN을 설계했다.

## Introduction  

AlexNet 이 소개된 이후 image recognition의 분야가 활발히 개발되었다. Action Recognition의 경우 I3D가 최상의 성능을 보여주지만 best hand-craft 방식인 iDT에 대한 개선 margine은 image recognition만큼 인상적이지 않다. 게다가, 개별 frame에서 동작하는 image 기반 2D CNN (ResNet-152) 의 경우는 Sport-1M banchmark에서 SOTA의 성능을 달성한다. 이러한 결과는 2D CNN이 비디오 분석에서 중요한 측면으로 간주되는 temporal 정보와 motion 패턴을 modeling할 수 없다는 점을 감안할 때 놀랍고 실망스럽다. 이러한 것을 볼 때 sequence에서 static frame에 이미 포함된 강력한 action class information 때문에 정확한 동작 인식을 위한 temporal reasoning이 필수적이지 않다고 가정할 수 있다.  

우리는 이러한 관점을 바탕으로 도전하며 3D CNN (spatiotemporal video volume에 대해 3D convolution을 수행하는 네트워크 ) 을 통해 action recognition에서 temporal reasoning의 역할을 다시 검토한다. 3D CNN은 많은 논문에서 다뤄진 반면에, 우리는 (static image 분야에서 좋은 방법으로 입증되었던) residual learning framework 내에서 이를 고려한다. 우리는 Kinetics나 Sport-1M dataset에 대해 학습 및 평가를 했을 때 3D ResNet이 동일한 깊이에서 2D ResNet보다 현저히 우수한 성능을 보여주는 것을 확인하였다.  

이러한 결과에 영감을 받아 2D (spatial convolution)의 극단과 full 3D의 중간 지점으로 볼 수 있는 두 가지 새로운 형태의 spatiotemporal convolution을 소개한다. 첫 번째 formulation은 network의 초기 layer에서만 3D convolution을 사용하고 top layer에서는 2D convolution을 사용하는 mixed convolution (MC) 라고 하는 것이다. 이 design의 근거는 motion modeling은 3D convolution을 통해 구현될 수 있는 low/mid-level 작업이고 이러한 mid-level motion feature ( top layer에서 2D convolution으로 구현됨 )에 대한 spatial reasoning은 정확한 action recognition으로 이어진다는 것이다. 우리는 MC ResNet이 비슷한 capacity의 2D ResNet에 비해 clip-level accuracy가 3~4%가 향상되고 parameter가 3배 더 많은 3D ResNet의 성능과 match됨을 보여준다. 두 번째 spatiotemporal variant는 "(2+1)D" block이다. 이건 3D convolution을 2개의 개별적이고 연속적인 연산인 2D spatial convolution과 1D temporal convolution으로 명시적으로 분해하는 걸 말한다. 그러면 이러한 분해로부터 두 가지 장점을 얻을 수 있다. 첫 번째 장점은 이러한 두가지 operation 사이에 추가적인 nonlinear rectification이다. 이는 더 복잡한 기능들을 표현할 수 있는 모델을 랜더링한다. (3D convolution을 사용하는 network에 비해 nonlinearity의 수를 효과적으로 늘리기 때문) 두 번째 잠재적인 이점은 분해가 optimization을 최적화하여 실제로 더 낮은 training loss와 test loss를 산출하다는 점이다. 즉, apperarnce와 dynamic이 함께 얽혀있는 full 3D filter에 비해 (2+1)D block ( temporal과 spatial compoenent로 분해된 ) 이 최적화하기 더 쉽다는 것을 알 수 있다. 우리의 실험은 모든 layer에서 (2+1)D block을 균일하게 채택한 ResNet이 Kinetics와 Sports-1M dataset 모두에서 SOTA 달성했다는 것을 보여준다.  


## Related Work  

Video를 이해하는 것은 computer vision에서 핵심적인 문제 중 하나며 이러한 연구에 대한 기여는 비디오 분석을 sptiotemporal feature를 개발하는데 중점을 두었다. 제안되었던 video representation들로는 STIP, SIFT-3D, HOG3D, Motion Boundary Histogram, Cuboids, Action Bank 등이 있다. 이러한 것들은 hand-crafted이며 histogram이나 pyramid 기반으로 하는 것과 같은  다른 feature encoding scheme을 사용한다. 이러한 hand-crafted representation 중 향상된 iDT(Dense Trajectiories)는 강력한 결과로 최신 기술로 간주된다.  

AlexNet 모델 도입 이후 많은 발전을 이루었다. 추가적으로 CNN에서 시간 정보를 융합하는 방법에 대한 연구도 이루어졌는데 [16] 논문은 시간상의 모든 conv layer의 연결을 확장하고 spatial convolution 외에 temporal convolution을 통해 activation을 계산하는 "slow fusion"을 제안했다. 하지만 그들은 개별 frame에서 동작하는 network가 비디오의 전체 spatiotemporal volume을 처리하는 network와 동등하게 수행된다는 것을 발견했다. Action Recognition에서 행동을 인식하기 위해 temporal convolution을 사용하는 [1], [5]에서 처음 제안되었다. .. (중략) .. 최근에는 3D CNN이 좋은 성능을 나타내었고 다른 작업에서도 잘 일반화되는 것으로 입증되었다.  

CNN 기반 비디오 modeling에서 다른 영향력있는 접근 방식은 [29]에서 도입한 2-stream framework로, optical flow에서 추출된 deep feature를 color RGB 입력에서 계산된 보다 깊은 CNN 활성화와 fusion할 것은 제안했다. [13]과 [9]를 사용하여 이러한 2-stream network를 강화하였다. .. (중략) .. 최근엔 I3D (2-stream과 3D convolution을 결합한 모델) 가 Kinetics dataset에서 SOTA다.  

우리는 3D CNN과 2-stream network, ResNet을 재검토한다. 이 연구에서는 frame에 대한 2D conv, clip에 대한 1. 2D conv, 2. 3D conv, 3. interleaved(mixed) 3D-2D conv, 4. 3D conv를 2D spatial convolution으로 분해한 다음 1D temporal convolution 한 것을 포함한다. 이걸(4번) (2+1)D convolution이라고 한다. ResNet 아키텍처 [13] 내에서 사용될 때 (2+1)D convolution은 action recognition의 4가지 다른 benchmark에서 SOTA 결과를 이끌어낸다. R(2+1)D라고 하는 우리의 아키텍처는 spatiotemporal convolution을 spatial 및 temporal convolution으로 분해하는 방식으로 $F_{ST}CN$ [33]과 관련이 있다. 그러나 $F_{ST}CN$ 은 하위 layer의 여러 spatial layer와 상단 두 개의 병렬 temporal layer로 구현된다. 반면 R(2+1)D는 layer 분해 (즉, spatiotemporal convolution을 spatial conv와 temporal conv로 분해하는 것)하는 것에 중점을 둔다. 결과적으로 R(2+1)D는 network에서 spatial 및 temporal convolution을 번갈아가며 나타낸다.  또한 R(2+1)D는  P3D[25]와 밀접한 관련이 있다 ([25]는 2D ResNet의 bottleneck block을 비디오에 적용하는 세 개의 서로 다른 residual block을 포함한다). block은 서로 다른 형태의 spatiotemporal convolution을 구현한다. spatial followed by temporal, spatial and temporal in parallel, and spatial followed by temporal spatial and temporal in parallel, and spatial followed by temporal with skip connection from apatial convolution to the output of the block, respectively. P3D model은 network 깊이를 통해 이 세 블록을 순서대로 interleaving하여 구성된다. 이와 다르게 R(2+1)D는 모든 layer에서 single type의 spatiotemporal residual block을 균일하게 사용하며 bottleneck 현상을 포함하지 않는다. 대신에 각 block의 spatial-temporal 분해에 대한 dimension을 신중히 선택함으로써 size가 작으면서도 SOTA action recognition accuracy를 달성하는 모델을 얻을 수 있음을 보여준다. 예를 들어, RGB를 입력으로 사용하는 Sport-1M에서 ResNet-34를 사용하는 R(2+1)D는 P3D가 ResNet-152를 사용함에도 불구하고 clip@1 accuracy에서 9.1%가 더 높다.(57.0% vs. 47.9%)  

## Convolutional residual blocks for video  

이 section에서는 residual learning framework 내의 몇몇 spatiotemporal convolution 변형에 대해 논의한다. Input clip x의 size는 3 x L x H x W (L: clip의 frame 수, H와W: frame의 height와  width, 3: RGB channel 수)이다. $z_{i}$ 는 residual network의 i번째 convolutional block에서 계산된 tensor라고 하자. 여기서는 "vanilla" residual block (bottleneck 없음) [13]만 고려하며, 각 block은 각 layer 다음에 ReLU activation function를 사용하는 두 개의 conv layer로 구성된다. 그 다음 i 번째 residual block의 출력은 $z_{i}$ = $z_{i-1}$ + $F(z{i-1};\theta_{i})$ 로 표현할 수 있다. 여기서 $F(;\theta_{i})$ 는 가중치 $\theta_{i}$ 와 ReLU function의 적용으로 parameterize된 두 convolution의 구성을 구현한다. 이 작업에서 우리는 conv residual block의 sequence가 전체 spatiotemporal volume에 대해 global average pooling을 수행하는 top layer와 최종 classification prediction을 담당하는 fully-connected layer로 마치는 network를 고려한다.  

<p align="center"><img src="/img/R(2+1)D-P1.JPG"></p>  


#### R2D : 2D convolutions over the entire clip  

비디오의 2D CNN은 비디오의 시간 순서를 무시하고 L frame을 channel과 유사하게 취급하기 때문에 입력 4D tensor x를 3LxHxW 크기의 3D tensor로 재구성하는 것으로 생각할 수 있다. i 번째 residual block의 출력 $z_{i}$ 도 3D tensor다. 이때 크기는 $N_{i}$ x $H_{i}$ x $W_{i}$ 다 ( $N_{i}$ 는 i 번째 block에 적용된 conv filter의 수, $H_{i},W_{i}$ 는 spatial dimension이고 pooling이나 striding으로 input frame의 size보다 작을 수 있음 ). 각 filter는 3D이며, size는 $N_{i-1}$ x $d$ x $d$ 다 ( $d$ 는 spatial width와 height). Filter는 3차원이지만 이전 tensor $z_{i-1}$ 의 spatial dimension에 대해 2D로만 convolution된다. 각 filter는 단일 channel output을 생성한다. 따라서 R2D의 첫 번째 conv layer는 단일 channel feature map에서 비디오의 전체 temporal 정보를 축소하여 후속 layer에서 발생하는 temporal reasoning을 방지한다. 이러한 유형의 CNN 아키텍처는 그림 1(a)에 나와있다. Feature map에는 temporal 의미가 없으므로 이 network에 대해 temporal stride를 수행하지 않는다.  

#### f-R2D : 2D convolutions over frames  

또 다른 2D CNN 접근 방식은 일련의 2D convolution residual block을 통해 L frame을 독립적으로 처리하는 방법이다. 모든 L frame에 동일한 filter가 적용된다. 이 경우, conv layer에서 temporal modeling이 수행되지 않으며 top의 global spatiotemporal pooling layer는 L frame에서 독립적으로 추출된 정보를 단순히 fuse한다. 이 아키텍처 variant를 f-R2D (frame-based R2D)라고 한다.  

#### R3D : 3D convolutions  

3D CNN들[15,36]은 temporal 정보를 보존하고 network layer를 통해 전파한다. tensor $z_{i}$ 는 이 경우 4D이고 size는 $N_{i}$ x $L$ x $H_{i}$ x $W_{i}$ 이다 ( $N_{i}$ 는 i 번째 block에 사용된 filter 수). 각 filter는 4차원이고 size는 $N_{i-1}$ x $t$ x $d$ x $d$ 이다 ( $t$ 는 filter의 temporal extent를 나타내며 이 work에서는 $t$ =3을 사용한다). Filter는 3D다 (즉, temporal과 spatial dimension에서 convolution된다). 이러한 유형의 CNN 아키텍처는 그림 1(d)이다.  

#### $MC_{x} and rMC_{x}$ : mixed 3D-2D convolutions  

한 가지 가설은 motion modeling (3D convolution을 말함)이 초기 layer에서 유용할 수 있는 반면, 더 higher level의 semantic abstraction (late layer)에서는 motion이나 temporal modeling이 필요하지 않다는 것이다. 따라서 그럴듯한 아키텍처는 3D convolution으로 시작하여 상위(top) layer에서 2D convolution을 사용하도록 전환하는 것이다. 이 작업에서는 5개의 conv group (표 1 참고)을 갖는 3D ResNet (R3D)을 고려하므로 첫 번째 변형은 group 5의 모든 3D convolution을 2D convolution으로 대체하는 것이다. 이 변형은 MC5 (Mixed Convolutions)로 표시된다. Group 4와 5에서 2D convolution을 사용하는 두 번째 변형을 설계하고 이 model의 이름을 MC4로 지정한다 (Group 4 및 더 deep layer에서 모든 convolution이 2D임을 의미). 이 패턴에 따라 MC3, MC2 변형도 만든다. MC1의 경우 clip input에 적용된 2D ResNet (fR2D)과 동일하므로 고려하지 않는다. 이러한 유형의 CNN 아키텍처는 그림 1(b)에 나와있다. 대체 가능한 가설은 temporal modeling이 2D convolution을 통해 apperance 정보를 초기에 캡처하는 방식으로 deep layer에서 더 유리할 수 있다는 것이다. 이러한 가능성을 설명하기 위해서 "Reversed" Mixed Convolutions를 실험한다. MC model의 명명 규칙에 따라 이러한 model을 rMC2, rMC3, rMC4, rMC5로 표시한다. 그러므로, rMC3는 block 1과 2의 2D conv와 group 3 및 더 deeper group의 3D conv를 포함한다. 이러한 유형의 아키텍처는 그림 1(c)에 설명되어 있다.  

<p align="center"><img src="/img/R(2+1)D-Table1.JPG"></p>  
<center>[Table 1]</center>  

#### R(2+1)D: (2+1)D convolutions  

또 다른 가능한 이론은 full 3D convolution이 2D convolution에 이어 1D convolution에 의해 보다 편하게 근사화되어 spatial 및 temporal modeling을 두 개의 개별 단계로 분해할 수 있다는 것이다. 따라서 R(2+1)D라는 network 아키텍처를 설계한다. 여기서 $N_{i-1}$ x $t$ x $d$ x $d$ 크기의 $N_{i}$ 3D conv filter를 $M_{i}$ 2D conv filter (size는 $N_{i-1}$ x 1 x $d$ x $d$ )와 $N_{i}$ temporal convolutional filter (size는 $M_{i}$ x $t$ x 1 x 1 )로 구성된 R(2+1)D로 대체한다. hyperparameter $M_{i}$ 는 spatial과 temporal convolution 사이에 signal이 project되는 중간 부분 subspace의 차원을 결정한다. 우리는 $M_{i}$ = $\left \lfloor \frac{td^{2}N_{i-1}N_{i}}{d^{2}N_{i-1}+tN_{i}} \right \rfloor$ 을 선택하여 (2+1)D block의 parameter 수가 full 3D convlution을 구현하는 것과 동일하다. 이 spatiotemporal 분해는 모든 3D convolutional layer에 적용될 수 있다. 이 분해에 대한 설명은 input tensor $z_{i-1}$ 이 single channel (즉, N_{i-1}=1)을 포함하는 단순화된 설정에 대한 그림 2에 설명한다. 3D conv에 spatial과 temporal striding (downsampling 구현)이 있는 경우 stride는 해당 spatial or temporal 차원으로 분해된다. 이 아키텍처는 그림 1(e)에 설명되어 있다.

<p align="center"><img src="/img/R(2+1)D-P2.JPG"></p>  

Full 3D conv와 비교할 때 (2+1)D 분해는 두 가지 장점을 가진다. 첫째, parameter 수를 변경하지 않았음에도 각 block에서 2D와 1D conv 간의 추가 ReLU로 인해 network의 nonlinearity의 수를 두 배로 늘린다. nonlinearity의 수를 늘리면 표현할 수 있는 feature의 복잡성이 증가한다. VGG network에서 언급했듯이 그 사이에 추가 nonlinearity이 있는 여러 개의 작은 filter를 적용하여 큰 filter의 효과를 근사한다. 두 번째 장점은 3D conv를 별도의 spatial 및 temporal component로 강제하면 최적화가 쉬어진다. 이는 동일한 capacity의 3D conv network에 비해 더 낮은 training error를 나타낸다. 이는 18(왼쪽)과 34(오른쪽) layer가 있는 R3D 및 R(2+1)D에 대한 training과 test error를 보여주는 그림 3에 나와있다. 동일한 수의 layer(or parameter)에 대해 R(2+1)D는 R3D에 비해 test error가 낮을 뿐 아니라 train error도 낮다는 것을 알 수 있다. 이것은 spatiotemporal filter가 분해될 때 최적화가 더 쉬워진다는 표시다. Training loss의 gap은 34개의 layer를 가진 network의 경우 특히 크다. 이는 deep해질수록 최적화의 용이성이 더 증가함을 의미한다.  

<p align="center"><img src="/img/R(2+1)D-P3.JPG"></p>  

우리의 분해는 R2D의 bottleneck block을 video classifiaction에 적용하기 위해 제안된 p3D(Pseudo-3D block)[25]와 관련이 있다. 3가지 다른 P3D가 제안되었다 (P3D-A, P3D-B, P3D-C). Block은 서로 다른 convolution 순서를 구현한다 (spatial followed by temporal, spatial and temporal in parallel, and spatial followed by temporal with skip connection from spatial convolution to the output of the block, respectively). (2+1)D conv는 P3D-A block과 가장 가깝게 관련있지만 bottleneck 현상이 있다. 또한 최종 P3D 아키텍처는 2D conv가 사용되는 첫 번째 layer를 제외하고 network 전체에서 이 세 block을 순서대로 interleaving하여 구성된다. 대신 모든 block에서 동일한 (2+1) 분해가 사용되는 homogeneous 아키텍처를 제안한다. 또 다른 차이점은 P3D-A는 3D conv와 parameter 수를 일치시키도록 의도적으로 설계하지 않았다는 점이다. R(2+1)D가 아미텍처에서 매우 단순하고 동질적이라는 사실에도 불구하고, 우리의 실험은 Sport-1M에서 R3D, R2D, P3D보다 우수한 성능을 보여준다. (표 4 참고)  

<p align="center"><img src="/img/R(2+1)D-Table4.JPG"></p>  

## Experiments  

이전 section에서 소개되었던 다른 sptiotemporal conv들에 대한 action recognition accuracy를 비교한다. Deep model을 scratch부터 학습할 수 있는 충분히 큰 dataset인 Kinetics와 Sports-1M dataset을 기본 benchmark로 사용하였다. 좋은 model은 다른 dataset에 대해서도 좋은 성능을 보여야 하므로 Kinetics와 Sport-1M로 pre-training 후 UCF-101와 HMDB51에서 fine-tuning한 결과를 보여준다.  

#### Experimental setup  
##### Network Architectures   
우수한 성능과 단순성으로 인해 실험을 deep residual network[13]로 제한한다. 표 1은 실험에서 고려한 두 가지 R3D 아키텍처 (3D ResNet)의 세부 사항을 제공한다. 첫 번째에는 18개의 layer가 있고 두 번째 변형에는 34개의 layer가 있다. 각 network는 112×112 크기의 L RGB frame으로 구성된 clip을 입력으로 사용한다. conv1에서 1×2×2의 convolution stride에 의해 구현된 하나의 spatial downsampling과 conv3_1, conv4_1 및 conv5_1에서 2×2×2의 convolution stride로 구현된 3개의 spatiotemporal downsampling을 사용한다. 이러한 R3D model에서 3D convolution을 각각 2D conv, mixed conv, reversed mixed conv 및 (2+1)D conv로 대체하여 아키텍처 R2D, MCx, rMCx 및 R(2+1)D를 얻는다. 우리의 spatiotemporal downsampling은 3D convolutional striding에 의해 구현되기 때문에 MCx 및 rMCx에서와 같이 3D convolution이 2D convolution으로 대체되면 spatiotemporal downsampling은 공간적으로 된다. 이 yield 차이는 마지막 conv layer에서 temporal size가 다른 activation tensor를 생성합니다. 예를 들어, f-R2D의 마지막 conv layer의 출력은 temporal striding이 적용되지 않았기 때문에 L×7×7 이다. 반대로 R3D 및 R(2+1)D의 경우 마지막 convolution tensor의 크기는 $\frac{L}{8}$ ×7×7 이다. MCx 및 rMCx model은 temporal striding이 적용되는 횟수에 따라 time dimension에서 다른 크기를 생성한다 (표 1 참고). 마지막 convolution layer에서 생성된 출력의 크기에 관계없이 각 network는 최종 convolution tensor에 global spatiotemporal average pooling을 적용한 다음, 최종 classification을 수행하는 fully-connected(fc) layer가 뒤따른다 (fc layer의 output 차원은 클래스 수 (예 : Kinetics의 경우 400).  

##### Training and evaluaion  
Kinetics dataset에 대한 initial evaluation을 수행한다. 공정한 비교를 위해 모든 network에 18개의 layer를 설정하고 동일한 입력에 대해 처음부터 훈련한다. 비디오 frame은 128×171의 ​​크기로 조정되고 각 clip은 112×112 크기의 window을 무작위로 잘라 생성된다. 학습하는 동안 temporal jittering을 사용하여 비디오에서 연속 L frame을 무작위로 sampling한다. 이 비교에서는 model이 8-frame clip (L=8) 및 16-frame clip (L=16)에 대해 학습되는 두 가지 설정으로 실험합니다. Batch Normalization는 모든 conv layer에 적용된다. GPU 당 32개 clip의 mini-batch size를 사용한다. Kinetics에는 약 24만 개의 training 비디오만 있지만 temporal jittering을 위해 epoch 크기를 1M으로 설정했다. 초기 learning rate는 0.01로 설정되고 10 epoch마다 10으로 나눈다. Distributed training에서 warm-up[12]을 위해 처음 10개 epoch을 사용한다. 학습은 45 epoch만큼 하였다. clip top-1 accuracy 및 비디오 top-1 accuracy를 측정했다. 비디오 top-1의 경우 비디오에서 균일하게 sampling된 10개 clip의 center crop을 사용하고 이러한 10개의 clip prediction을 평균하여 비디오 prediction을 얻는다. 학습은 caffe2를 사용하는 GPU cluster에서 synchronous distributed SGD로 수행된다[3].  

#### Comparison of spatiotemporal convolutions  

<p align="center"><img src="/img/R(2+1)D-Table2.JPG"></p>  

Table 2는 Kinetics에 대한 clip top-1, 비디오 top-1 action classification accuracy를 보여준다. 추론할 수 있는 몇 가지 결과는 다음과 같다. 첫째, 2D ResNet (f-R2D, R2D)과 R3D, mixed conv model (MCx, rMCx)의 성능의 차이가 크다. 이 차이는 8-frame input setting에서 1.3 ~ 4%이며 16-frame일 때는 더 크다. 이는 action recognition에 motion modeling이 중요하다는 것을 의미한다. .. (중략) .. . R(2+1)D가 8-frame setting에서 MCx, rMCx, R3D보다 2.1 ~ 3.4%, 16-frame input setting에서 3.1 ~ 4.7% 더 우수하다. 이는 별도의 spatial과 temporal convolution에서 3D convolution을 분해하는 것이 공동으로 또는 mixed 3D-2D convolution을 통해 spatiotemporal 정보를 modeling하는 것보다 낫다는 것을 나타낸다.  

<p align="center"><img src="/img/R(2+1)D-P4.JPG"></p>  

Figure 4는 Kinetics에서 계산 복잡도 (FLOP) 대비 비디오 top-1 accuracy를 보여준다. 그림 4(a)는 8-clip, 4(b)는 16-clip에서 학습된 model을 나타낸다. 가장 효율적인건 R2D지만 accuracy는 가장 낮다. 실제로 R2D는 conv1 이후 temporal dimension을 축소하므로 f-R2D보다 약 7배 빠르다. Accuracy 측면에서 보면 R2D가 8-frame clip에서 학습할 땐 fR2D와 유사한 성능이지만 16-frame clip에서는 1.6% 더 낮다. 이는 R2D가 conv1 layer에서만 temporal modeling을 수행해서 더 긴 clip input을 처리하기 때문이다. 흥미로운 건 rMC3는 conv3_1에서 일시적인 striding을 수행하므로 모든 subsequent 2D convolutional layer에서 더 작은 activation tensor를 생성하므로 f-R2D보다 효율적이다 (f-R2D는 모든 frame을 독립적으로 처리, temporal stride를 수행하지 않음). rMC2는 group 2에서 2D conv를 사용하고 group 3에서 temporal stride를 수행하지 않아서 rMC3보다 cost가 크다. R(2+1)D는 R3D와 비슷한 computational cost를 갖지만 accuracy는 더 높다. (중략)  

##### Why are (2+1)D convolutions better than 3D?  

Figure 3은 18개 layer(왼쪽) 및 34개 layer(오른쪽)를 사용하여 R3D 및 R(2+1)D의 Kinetics에 대한 학습과 테스트 error를 보여준다. 우리는 이미 R(2+1)D가 R3D보다 더 낮은 테스트 error를 제공한다는 것을 알고 있지만 이 plot에서 흥미로운 건 R(2+1)D가 더 낮은 학습 error를 생성한다는 것이다. R3D에 비해 R(2+1)D에 대한 학습 error의 감소는 특히 34개의 layer가 있는 아키텍처에서 두드러진다. 이것은 특히 깊이가 증가함에 따라 R(2+1)D의 spatiotemporal 분해가 R3D에 비해 최적화를 더 쉽게 rendering함을 시사한다.  

#### Revisiting practices for video-level prediction  

[37]은 LTC(long-term convolutions)를 사용해 더 긴 input clip(100 frame처럼)에서 비디오 CNN을 학습하여 accuracy 향상을 얻을 수 있음을 보여주었다. 이 아이디어를 검토하고 다양한 frame (8, 16, 24, 32, 40, 48)의 다양한 input clip을 가진 18개 layer의 R(2+1)D를 사용해서 Kinetics에서 평가한다. 이러한 network의 마지막 conv layer의 출력은 temporal size가 각 다르지만 global spatiotemporal average pooling을 사용해 fully-connected layer에 제공되는 고정된 size 표현을 생성한다. Pooling에는 parameter가 포함되지 않기 때문에 동일한 parameter 수가 있다. 단순히 다른 길이의 input을 보는 것이다. 그림 5 a)는 Kinetics valid set을 서로 다른 길이의 input을 사용했을 때 clip-level과 video-level accuracy를 보여준다. 비디오 level prediction은 비디오에서 균등한 간격으로 10개의 clip에 대해 얻은 clip level prediction을 평균하여 수행된다. 한 가지 흥미로운 점은 frame을 추가하면 clip accuracy가 계속 증가하지만 비디오 정확도는 32-frame에서 최고조에 달한다는 것이다. 이러한 네트워크에는 동일한 수의 매개 변수가 있다 (풀링에는 학습 가능한 매개 변수가 포함되지 않기 때문에). 그들은 단순히 다른 길이의 입력을 본다. 그림 5 a)는 다른 입력 길이와 관련하여 Kinetics 검증 세트에 대한 클립 수준 및 비디오 수준 정확도를 플롯한다.  

<p align="center"><img src="/img/R(2+1)D-P5.JPG"></p>  

이러한 model은 모두 동일한 parameter 수를 가지고 있기 때문에 "video-level accuacy가 차이가 나는 원인이 무엇인가?"라고 묻는 것이 당연하다. 이 문제를 해결하기 위해 두 가지 실험을 진행하였다. 첫 번째 실험은 8-frame clip에서 학습된 model을 가져와서 32-clip을 input으로 사용하여 test한다. 결과적으로 clip accuracy와 video accuracy가 1.2, 5.8% 감소하였다. 두 번째 실험은 8-frame model의 parameter를 초기화하여 32-frame model을 fine-tuning했다. 그 결과는 32-clip에서 처음부터 학습할 때랑 거의 같은 결과를 얻었고 8-frame model에 비해 7%의 향상을 생성한다. 그러나 8-frame model이 FLOP 측면에서 32-frame보다 7.3배 빠르기 때문에 8-frame network에서 32-frame model을 fine-tuning하면 학습시간이 많이 단축된다는 장점이 있다. ..(중략).. Table 3은 다양한 길이의 clip에서 학습과 평가된 18 layer를 사용하여 R(2+1)D의 총 학습 시간과 accuracy를 나타낸다.  

<p align="center"><img src="/img/R(2+1)D-Table3.JPG"></p>  

##### How many clips are needed for accurate video-level prediction?  
Figure 5 b)는 32-frame clip에서 학습된 R(2+1)D model의 18 layer를 clip 수를 변경할 때 top-1 accuracy를 비교한 그림이다. 20개의 frame을 사용한 것이 100개의 frame을 사용했을 때보다 0.5%가 낮지만 prediction은 5배 빠르다.

#### Action recognition with a 34-layer R(2+1)D net  
이 섹션에서는 R(2+1)D-34로 표시하는 34-layer version의 R(2+1)D를 사용하여 결과를 보여준다. 아키텍처는 Table 1의 오른쪽 열에 표시된 것과 동일하지만 3D convolution이 (2+1)D에서 spatiotemporal로 분해됩니다. 우리는 RGB 및 optical flow 입력 모두에 대해 R(2+1)D 아키텍처를 학습시키고 two-stream framework[29] 및 subsequent work[4, 9]에서 제안된대로 평균화하여 prediction score를 fusion합니다. 효율성 때문에 Farneback's method[8]를 사용하여 optical flow를 계산한다.  

##### Dataset  
제안된 R(2+1)D 아키텍처를 4개의 공개 benchmark에서 평가한다.  
+ Sports-1M  
  - 스포츠 비디오 분류를 위한 대규모 데이터 세트  
  - 487개의 세분화된 스포츠 카테고리의 110만 개의 동영상 포함  
  - train/test split 제공  

+ Kinetics  
  - 400 개의 인간 행동에 대한 약 300K 동영상  
  - test set의 주석이 공개되지 않았으므로 validation set에 대한 결과를 보고  

+ UCF101 및 HMDB51  
  - action recognition에 대한 잘 확립 된 benchmark  
  - UCF101은 101개 카테고리의 약 13K 동영상  
  - HMDB51은 51개 클래스의 6K 동영상  
  - train/test를 위한 3개의 split이 제공
  - 3개 split 모두에 대해 평균화하여 정확도를 보고

표에 대한 분석 내용은 따로 설명하지 않겠습니다. 논문 참고하시길 바랍니다!  

<p align="center"><img src="/img/R(2+1)D-Tables.jpg"></p>  

#### Conclusions  

우리는 비디오에서 action recognition을 위해 다양한 spatiotemporal convolution의 효과에 대한 실증적 연구를 제시했다. 우리가 제안한 아키텍처 R(2+1)D는 Sports1M, Kinetics, UCF101 및 HMDB51의 최신 기술과 비슷하거나 우수한 결과를 달성한다. 우리의 분석이 spatiotemporal convolution의 잠재적인 효능과 모델링 유연성을 활용하는 새로운 network 설계에 영감을 줄 수 있기를 바란다. 우리의 연구는 단일 유형의 network (ResNet)와 (2+1)D spatiotemporal 분해의 동질적인 사용에 초점을 맞추었지만 향후 작업은 우리의 접근 방식에 더 적합한 아키텍처를 검색하는 데 전념할 것이다.  

#### Appendix
Filter를 visualization한 결과를 보여주고 있음.  

<p align="center"><img src="/img/R(2+1)D-Appendix.JPG"></p>  
