---
layout: single
title: "머신러닝2"
categories:
  - Python
tags:
  - jeykll
  - github
  - Minimalmistakes
  - blog
  - Python
---
# linear separable 2
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from mpl_toolkits.mplot3d import Axes3D

plt.close("all")

[X,Y] = datasets.make_moons(n_samples = 200, shuffle = True , noise = 0.2, random_state = 15) # 애는 형태를 정하는거
#여기서 noise를 넣으면 노이즈가 낌
# X는 2차원 data고 Y 는 label임.
#normalization 하자
scaler = StandardScaler()
scaler.fit(X)
X_std = scaler.transform(X)

[X_train, X_test, Y_train, Y_test] = train_test_split(X_std,Y, test_size = 0.4, random_state = 10, shuffle =True)
# 이거는 40퍼센트는 테스트로 두고 나머지 60%를 트레이닝으로 두는 거 의미

#이게 뭔지 그림으로 보자

df_clf = pd.DataFrame(np.c_[X_std,Y]) # 합치고
df_clf.columns = ["x0", "x1", "target"] # 컬럼스에 하나씩 넣는다.(이름을)
df_group = df_clf.groupby("target") # 그룹핑을 타겟으로 하면 됨

f1 = plt.figure(1)
#ax1 = f1.add_subplot(211, projection = '3d')
#ax2 = f1.add_subplot(212, projection = '3d')
ax1 = f1.add_subplot(111)  # 2차원

# ax1 = f1.add_subplot(211)
# ax2 = f1.add_subplot(212) 위아래로 그리는거 (몇바이 몇이고 몇번째)

# 데이터 그룹이 2개으 ㅣ인자를 받는다고 했다
# 0 or 1로 라벨링하니까 target이 된다

for target, group in df_group:
    #for 문이 1번 2번있는거다.
    #df 그룹은 2번 돌아가도록 되는거
    ax1.plot(group.x0, group.x1, '.', label = "target") # 이렇게 하면 2차원 플랏 나옴
    #전에서 선 은 h(x)의 거리를 말한거?
    #...
    #3차원 플랏을 해보자(구글링 python 3d plot)
    #ax1.plot(group.x0**2, group.x1*group.x1, group.x1**2, '.',label = "target") # 얘 해보면 분리 불가능한거를 확인가능
    #4차원은 그릴 수 없으니까 일부만 확인 가능
    # ax1.plot(group.x0**3, group.x0*(group.x1**2), (group.x0**2)*(group.x1), '.',label = "target")# 얘는 좀 되는듯?
    # 다른 걸르 또 그려보자.
    # ax1.plot(group.x0*(group.x1**2), (group.x0*2)*(group.x1), group.x1**3, '.',label = "target") # 얘는 더 가능성 커보이네

    #지금 하는건 4차원을 못그리니까 3차원 여러개로 4차원을 표현ㅇ하는거다.
    # 이거는 3차원을 각 평면에서 봐서 2차원 여러개로 보는거랑 마찬가지임.
    #4 차원에서 보면??
    #ax2.plot(group.x1**4, (group.x0**3)*(group.x1),group.x0**4,'.',label="target")
    #축이 많아이졈 합쳐져서 평면 하나가 비집고 들어갈 축이 늘어나지 않을까

svm_clf = SVC(C=0.1, kernel="poly", degree = 5, gamma = 1, coef0 = 1) # 폴리노미얼 커널을 불러옴
# svm_clf = SVC(C=0.1, kernel="rbf,  gamma = 1) # RBF를 불러옴

# 감마, 디그리, 코이피션트를 알아야 함. 이게 변수야
# 감마는 폴리 식에 감마랑 완전
#  degree가 크면 더 정하게 나눈다.(오버피팅할 여지도 있음)
# c는 소프트 마진의 정도

#이렇게 모델을 생성해볼게.
# 여기까지 한거는 컨널트릭을 사용한 svm 껍데기를 불러온 거

#svm_clf.fit(X_std, Y)

svm_clf.fit(X_train, Y_train) # 학습할 때

# 저번에 등고선(컨투어)로 그렸었지

# -2,-2 부터 2,2까지 등고ㄴ서 다 그려야 함

delta = 0.01
[xx0_min, xx0_max] = [min(X_std[:,0])-10*delta, max(X_std[:,0]+10*delta)]
[xx1_min, xx1_max] = [min(X_std[:,1])-10*delta, max(X_std[:,1]+10*delta)]
[xx0, xx1] = np.meshgrid(np.arange(xx0_min,xx0_max,delta), np.arange(xx1_min,xx1_max,delta))

#이거를 디시전 펑션에 넣는ㄷ

h = svm_clf.decision_function(np.c_[xx0.ravel(), xx1.ravel()])
h = h.reshape(xx0.shape)

ax1.clabel(ax1.contour(xx0,xx1, h, cmap=plt.cm.twilight))

# svm은 리니어밖에 못하는데 곡선으로 표현했다. 4차원 공간을 갔다와서 그럼
# 커널 트릭을 도입했다는건 차원이 다른 문제다.

# 그러면 score를 구해봅시다.

print(svm_clf.score(X_test, Y_test))
#svm_clf에서 rbf, poly등 이랑 감마 디그리 등을 바꾸면서 하면 스코어 다르게 나온다.
#얘는 폴리가 더 좋게 나옴
```

# HW3
```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from mpl_toolkits.mplot3d import Axes3D


plt.close("all")

#[X,Y] = datasets.make_moons(n_samples = 200, shuffle = True , noise = 0.2, random_state = 10)
[X,Y] = datasets.make_circles(n_samples = 200, shuffle = True , noise = 0.2, random_state = 10, factor = 0)
#[X,Y] = datasets.make_circles(n_samples = 200, shuffle = True , noise = 0.2, random_state = 10, factor = 0.5)

#normalization
scaler = StandardScaler()
scaler.fit(X)
X_std = scaler.transform(X)

[X_train, X_test, Y_train, Y_test] = train_test_split(X_std,Y, test_size = 0.4, random_state = 10, shuffle =True)

df_clf = pd.DataFrame(np.c_[X_std,Y])
df_clf.columns = ["x0", "x1", "target"]
df_group = df_clf.groupby("target")

f1 = plt.figure(1)
ax1 = f1.add_subplot(111)

for target, group in df_group:
    ax1.plot(group.x0, group.x1, '.', label = "target")



svm_clf = SVC(C=0.1, kernel="rbf",  gamma = 1)
#svm_clf = SVC(C=0.1, kernel="poly", degree = 5, gamma = 1, coef0 = 1)


svm_clf.fit(X_train, Y_train)

delta = 0.01
[xx0_min, xx0_max] = [min(X_std[:,0])-10*delta, max(X_std[:,0]+10*delta)]
[xx1_min, xx1_max] = [min(X_std[:,1])-10*delta, max(X_std[:,1]+10*delta)]
[xx0, xx1] = np.meshgrid(np.arange(xx0_min,xx0_max,delta), np.arange(xx1_min,xx1_max,delta))


h = svm_clf.decision_function(np.c_[xx0.ravel(), xx1.ravel()])
h = h.reshape(xx0.shape)

ax1.clabel(ax1.contour(xx0,xx1, h, cmap=plt.cm.twilight))

print(svm_clf.score(X_test, Y_test))
```
