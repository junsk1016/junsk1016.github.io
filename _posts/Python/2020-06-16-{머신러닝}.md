---
layout: single
title: "머신러닝"
categories:
  - Python
tags:
  - jeykll
  - github
  - Minimalmistakes
  - blog
  - Python
---


# HW#1
```python
import numpy as np

x = [[1,2],[3,4]]
y = [[5,6],[7,8]]
X = np.array(x)
Y = np.array(y)

# 1)np.dot(x,y) : 내적
N1 = np.dot(X,Y)
print(N1)

# 2) np.diag : 대각행렬
N2_X = np.diag(X)
print(N2_X)
N2_Y = np.diag(Y)
print(N2_Y)

# 3) np.trace : 대각합
N3_X = np.trace(X)
print(N3_X)
N3_Y = np.trace(Y)
print(N3_Y)

# 4) np.linalg.det : 행렬식
N4_X = np.linalg.det(X)
print(N4_X)
N4_Y = np.linalg.det(Y)
print(N4_Y)

# 5) np.linalg.inv : 역행렬
N5_X = np.linalg.inv(X)
print(N5_X)
N5_Y = np.linalg.inv(Y)
print(N5_Y)


# 6) np.linalg.svd : 특이값 분해
N6_X = np.linalg.svd(X)
print(N6_X)
N6_Y = np.linalg.svd(Y)
print(N6_Y)

# 7) np.linalg.solve : 연립방정식 해 풀기
N7 = np.linalg.solve(X,Y)
print(N7)
```

# KNN
```python
from sklearn.datasets import load_iris
iris = load_iris()

from sklearn.datasets import load_breast_cancer
bCancer = load_breast_cancer()

# 일부만 학습하고 나머지는 test로 쓴다

from sklearn.model_selection import train_test_split
# data는 데이타고 타겟은 정답
X = iris.data
y = iris.target
# data 막 섞기
#test_size가 비율이다. 60개는 테스트로 들어가고 나머지 90개는 학습으로 들어간다.
# 랜덤 스테이트는 42번째의 랜덤한 특징이 있다.그냥 섞는 방법
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.4,random_state = 42)

#K-NN 사용해보자
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics # 평가하는 방법들이 들어가 있음

knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, y_train)#학습하는거

y_pred = knn.predict(X_test)
scores = metrics.accuracy_score(y_test, y_pred) # 정답에  해당하는걸 앞에 놓고 뒤에는 예측한걸 넣고 비교해서 score 알려준 것
```

# logistic regression
```python
import numpy as np
import pandas as pd
import matplotlib.pylab as plt

dfLoad = pd.read_csv('https://sites.google.com/site/vlsicir/testData_workHour_vs_passFail.txt', sep="\s+")
xx_Raw = np.array(dfLoad.values[:,0])
yy_Raw = np.array(dfLoad.values[:,1])
plt.plot(xx_Raw, yy_Raw, 'k.')

def sigmoid(x):
    return 1.0/(1+np.exp(-x))

#xxTest = np.linspace(-10, 10, num = 101)
#plt.plot(xxTest, sigmoid(xxTest), "k-")

N = len(xx_Raw) # x0-를 1로 패딩 해야 함
x_bias = np.c_[np.ones([N,1]), xx_Raw].T #Padding ones for x0
y = yy_Raw.reshape(N,1)
X = x_bias.T

eta = 0.1
n_iterations = 1000
wGD = np.zeros([2,1])   #initialized to 0
wGDbuffer = np.zeros([2,n_iterations+1]) # 매 스테이지 별로 wGD를 보려고 저장하는 용도

for iteration in range(n_iterations):
    mu = sigmoid(wGD.T.dot(x_bias)).T
    gradients = X.T.dot(mu-y)
    #gradients = - xHeight_bias.dot(sGD)
    #gradients = - (2/N)*(xx_bias.T.dot(yy-xx_bias.dot(wGD)))
    #gradients = - (2/N)*(xHeight_bias.T.dot(yWeight-wHeight_bias.dot(wGD)))
    wGD = wGD - eta*gradients
    wGDbuffer[:,iteration+1] = [wGD[0],wGD[1]]

xxTest = np.linspace(0,10,num=N).reshape(N,1)
xxTest_bias = np.c_[np.ones([N,1]), xxTest]
aaa = sigmoid(wGD.T.dot(xxTest_bias.T))
#plt.plot(aaa)
plt.plot(xxTest, sigmoid(wGD.T.dot(xxTest_bias.T)).T, "r-.")

print("Done")

#이걸로  probabilty mass function을 구할 수 있따. 1하나 붙여서 시그모이드에 넣으면.. 그거가 1일 확률을 명확히 알려줌
# 분류할 때는 시그모이드 사용. w는 NLL을 잘 정해서 그걸 미니마이제이션하는 방향으로 함
#가장 옵티멀한 디시전 크리에테이션을 구할 수 있다. wgdaㅏㄴ 있고 이거가 시그모이ㄱ드로 ㄷ르어가는ㄱ너만
#알고있으면 클래서 1일 확률등 어디에 속할 확률 구할 수 있따.
```

# DataFrame1
```python
import pandas as pd
import numpy as np

dfLoad = pd.read_csv("https://sites.google.com/site/vlsicir/ClassificationSample.txt", sep = '\s+' )
np.random.seed(4)
#Z = np.random.rand(580) >0.5 # 이렇게 >0.5로 하면 True, False로 나온다

Z = np.round(np.random.rand(580) >0.5) # round는 반올림
X = np.array(dfLoad["X"])
Y = np.array(dfLoad["Y"])

npCluster = np.c_[X,Y,Z]

#얘를 데이터 프레임화할거다
dfCluster = pd.DataFrame(npCluster)
dfCluster.columns = ["X","Y","Z"]
#Z기준으로 그룹핑할거다
dfGroup = dfCluster.groupby("Z")

for a, b in dfGroup:
    print(a) # 그룹의 name (0이였다가 1이 되는 애들)
    print(b) # group (dataframe이다)

#for name, group in dfGroup:
#    print(name) # 그룹의 name (0이였다가 1이 되는 애들)
#    print(group) # group (dataframe이다)
# 이게 그 의미임
```

# DataFrame2
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

plt.close("all")

dfLoad = pd.read_csv("https://sites.google.com/site/vlsicir/ClassificationSample.txt", sep = '\s+' )

samples = np.array(dfLoad)

X = samples[:,0]
Y = samples[:,1]

N = len(samples)
numX = 2 # 0 or  1

f1 = plt.figure(1)
ax1 = f1.add_subplot(111)
ax1.plot(X,Y,"b.")
ax1.set_aspect("equal")

#사이키런에 있지만 만들어보겠따

#일단 이니셜라이즈
mx = np.mean(X)
my = np.mean(Y)

sx = np.std(X)
sy = np.std(Y)

#initialize latent variables
z0 = np.array([mx-2*sx, my-2*sx]).reshape(1,2) # initial point 그리고 low방향으로 추가
z1 = np.array([mx+2*sx, my+2*sx]).reshape(1,2) # initial point
Z = np.r_[z0,z1]

ax1.plot(Z[0,0], Z[0,1], "r*")
ax1.plot(Z[1,0], Z[1,1], "r*")

# 모든 x_i를 리얼리스틱으로 매핑

cluster = np.round(np.random.rand(580) >0.5) # random하게 매핑

#for iteration in range(5): # 몇번 돌리냐에 따라 다름
#이제 바뀌지 않을 때까지 돌린다고 하자
iteration = 0
while True:
    iteration = iteration + 1 # 몇 번 도는지 체크
    # M step
    clusterOld = np.copy(cluster)# backup할 떄는 copy로 받아야 함
    for i in range(N):
        cluster[i] = np.linalg.norm(samples[i,:]-Z[0,:]) > np.linalg.norm(samples[i,:]-Z[1,:])
        #참이면 1로 매핑, 거짓이면 0으로 매핑
    if(np.alltrue(clusterOld==cluster)):
       break
    #옆으로 합침
    dfCluster = pd.DataFrame(np.c_[X, Y, cluster])
    dfCluster. columns = ["X", "Y","cluster"]
    dfGroup = dfCluster.groupby("cluster")
    #그루핑이 됬으니까 이제 민을 찾을 수 있다.

    # E step
    for j in range(numX):
        Z[j,:] = dfGroup.mean().iloc[j]

f2 = plt.figure(2)
ax2 = f2.add_subplot(111)
ax2.plot(Z[:,0],Z[:,1], "r*")
ax2.set_aspect('equal')#아직은 data가 없다
ax2.plot(X,Y,"b.")
for clusterName, group in dfGroup:
    ax2.plot(group.X,group.Y, ".", label = clusterName)

ax2.legend()  # 범례 추가

    #두번 째 파일애들은 ㄲ똑같이 dfLoad에만 바꾸며ㄴ ㅌ된다. 근데 얘는 부작용처럼 그룹을 이상하게 나눈다 그래서 여기다가는 가우시안 믹스쳐를 써야 할 수 있다.

#이게 한번 해본거다.
```

# Mixture

```python
# 가우시안 Mxiture 풀어보기
#이전 시간에 clustering이 매우 이상하게 됨

# 길게 되는건 코릴레이션 가우시안으로 풀 수 있음을 배웠다.
# 실제로 코릴레이트 가우시안 베리어블로 만날 확률이 매우 높다.
# clustering은 정답이 없다. 비슷한 애들끼리 묶는것. K값은 알아야 함.
# 이전에는 거리의 합이 최소 각각에 대해서.(EM Algorithm)
#  이번엔 가우시안 Mixture


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import scipy as sp
import scipy.stats # gaussian pdf 있다

plt.close("all") # 프로그램 시작할 때마다 다 닫히고 시작함

dfLoad = pd.read_csv("https://sites.google.com/site/vlsicir/ClassificationSample2.txt", sep = '\s+' )
samples = np.array(dfLoad)
X = samples[:,0]
Y = samples[:,1]

# 처음 이니셜은 임의로
N = len(X) # N = 580개인 것을 확인
numK = 2 #clustering 개수는 2개로 설정

#Initialize categorial distribution.
pi = np.ones([1, numK])*(1/numK) # 1,1로 하면 categorial dist가 아니다. 카테고는 다 더해서 1이 나와야
# ones는 1로 채울거다 앞 부분은 행이 1이고  numK만큼 열이 만들어진다는 읨
# 뒤에는 합이 1이여야되니까 numK만큼 나눠주게 되면 합이 1이 된다.
#이제 뮤랑 시그마K도 이니셜라이즈가 필요

#Initialize u and sigma
uX = np.mean(X) #각각의 평균을 보자
uY = np.mean(Y) #
sX = np.std(X) # 표준편차도 보자
sY = np.std(Y)

#대충 이근처로 할건데 너무 쉽게하면 알고리즘이 금방 찾으니까 좀 다르게 하자

#둘 다 2차원 벡터임, initialize하자
u0 = np.array([uX - sX, uY + sY]) #cluster 0 mean
u1 = np.array([uX + sX, uY - sY]) #cluster 1 mean

#그림으로 봐보자
f1 = plt.figure(1)
ax1 = f1.add_subplot(111)
ax1.plot(X, Y, "b.")
ax1.set_aspect("equal")
#ax1.plot(u0[0],u0[1],"r*")
#ax1.plot(u1[0],u1[1],"g*")


S0 = np.array([[sX*sX/4, 0],[0, sY*sY/4]]) #covarience는 없다고 가정 앞과 끝은 분산
S1 = np.array([[sX*sX/4, 0],[0, sY*sY/4]])

# 이제 또 responsibility  초기화 필요 이거는 클러스터1이나 2에 속할 확률을 K에 대해 정의한 것

R = np.ones([N, numK])*(1/numK) # R = (280, 2)

#M step부터 보자
#pi를 업데이트 하자 #파이는 0일때비중하고 1일때 비중만 있다.
#pi0 = (1/N)*(np.sum(R[:,0]) # R을 정의할 때 행으로 낭려하고 열로 w0와 w1을 선언했으니까 R[:,0]으로 설정
#근데 파이를 매트릭스로 설정해야 함

j = 0
while True:
    j = j + 1
    pi = [(1/N)*(np.sum(R[:,0])), (1/N)*(np.sum(R[:,1]))]

    # 파이만 있지 가우시안 value는 없으니까  R을 초기화시키자
    N0 = sp.stats.multivariate_normal.pdf(samples, u0,S0) # samples는 2차원 벡터 (580*2)개를 넣어야 함 samples가 580*2니까
    N1 = sp.stats.multivariate_normal.pdf(samples, u1,S1)


    #r0 = pi[0]*N0/(pi[0]*N0 + pi[1]*N1)
    #r1 = pi[1]*N0/(pi[0]*N0 + pi[1]*N1)
    #R = [r0, r1]니까
    #R = np.array([pi[0]*N0/(pi[0]*N0 + pi[1]*N1), pi[1]*N1/(pi[0]*N0 + pi[1]*N1)]).T

    #if(j == 20): #Converge condition 20번 돌고 멈추게하는거
     #   break  근데 차이를 보고 멈추게 하려면 아래처럼 한다(위의 R을 주석처리하고)
    Rold = np.copy(R)
    R = np.array([pi[0]*N0/(pi[0]*N0 + pi[1]*N1), pi[1]*N1/(pi[0]*N0 + pi[1]*N1)]).T
    if(np.linalg.norm(R-Rold)<0.001*N*numK): #No change
        break
    # u랑 구해야 함
    # r0에 대해서 밑으로 1000개 있음 이걸 x랑 곱해야 함
    wegihtedSum = samples.T.dot(R) # 2 (dimensional of data x 2 cluster#) matrix

    # 파이만 있지 가우시안 value는 없으니까  R을 초기화시키자
    u0 = wegihtedSum[:,0]/sum(R[:,0])
    u1 = wegihtedSum[:,1]/sum(R[:,1])
    S0 = samples.T.dot(np.multiply(R[:,0].reshape(N,1), samples))/sum(R[:,0]) - u0.reshape(2,1)*u0.reshape(2,1).T #뒤에 행렬이니까 reshape필요
    S1 = samples.T.dot(np.multiply(R[:,1].reshape(N,1), samples))/sum(R[:,1]) - u1.reshape(2,1)*u1.reshape(2,1).T

#groupby 하자
clusterCol = np.round(1 - R)[:, 0] # 1번째 열은 클러스터에 해당
dfCluster = pd.DataFrame(np.c_[samples, clusterCol])
dfCluster.columns = ["X", "Y", "K"]
dfGroup = dfCluster.groupby("K")

f2 = plt.figure(2)
ax2 = f1.add_subplot(111)


for cluster, dataGroup in dfGroup:
    ax2.plot(dataGroup.X , dataGroup.Y, "*", label=cluster)
    ax2.set_aspect("equal")
    ax2.plot(u0[0],u0[1],"r*")
    ax2.plot(u1[0],u1[1],"g*")

#f2 = plt.figure(2)
#ax2 = f1.add_subplot(111)
#ax2.plot(X, Y, "b.")
#ax2.set_aspect("equal")
#ax2.plot(u0[0],u0[1],"r*")
#ax2.plot(u1[0],u1[1],"g*")

#while True:

    #if():# Converge conndition
        #break #Converge
```

# Linear Separable

```python
#linear separable data를 먼저 해보자
#사이키런으로 했었잖아

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC # classfier하는거
from sklearn.model_selection import train_test_split
plt.close("all")

iris = datasets.load_iris()
# sepal : 꽃밭 petal : 꽃잎 (cm , 50개씩 0: setosa, 1: versicolor, 2: virginica labeling)
# 1번하고 2번은 잘 한다. 해보자.


X = iris["data"][0:100,(2,3)] #1번하고 2번만 가져옴(꽃잎의 너비랑ㄱ ㅣㄹ이만)
Y = iris["target"][0:100]
# 가져와서 그림을 그려보자

#standardization하는 거
scaler = StandardScaler()
scaler.fit(X)
X_std = scaler.transform(X)



f1 = plt.figure(1)
ax1 = f1.add_subplot(111)


df_clf = pd.DataFrame(np.c_[X_std,Y]) # 컬럼방향으로 합쳐서 표현
df_clf.columns = ["petalLength","petalWidth","target"] # 이름 넣기
# groupby 하자
df_clf_group = df_clf.groupby("target")

for target, group in df_clf_group:
    #그림을 2번 그리는거ㅏㄷ. 0번 라벨링에서 한번 1번 라벨링에서 한번
    ax1.plot(group.petalLength, group.petalWidth, '*', label="target")

# 아래 파란색이 0번이고 위가 1번이다 차이가 딱 난다. 그래서 이거는 노말라이제이션 해야한다.
# 2 데이터가 페어하게 처리하도록.
# x,y  data를 최대 최소로 처리하면 노말라이제이션이라고 하고
# 노말라이제이션은 max가 너무 크면 데이터 왜곡 발생ㄱ ㅏㄴ으성 있음 0~1로 매핑
# 평균에서 빼고 표준편차로 나누는 건 standardization이라ㅗ고 함 얘는 0~1로 픽스되지는 않음

# standardization 사용해보자 위에 추가함

svm_clf = SVC(C=0.01,kernel="linear")
svm_clf.fit(X_std,Y)
# svm_clf.decision_function([[0.5,0.5]]) #마진을 0으로 잡고 얼마나 멀어진지? 값 알려줌?
# 이 디씨전 펑션이 h(x)?
# svm_clf.predict([[-1,-1]])

# 바운더리를 보려면 등고선을 그려야함. meshgrid에 있음
delta=0.01
[xx0_min, xx0_max] = [min(X_std[:,0])-10*delta, max(X_std[:,0]+10*delta)]
[xx1_min, xx1_max] = [min(X_std[:,1])-10*delta, max(X_std[:,1]+10*delta)]
[xx0, xx1] = np.meshgrid(np.arange(xx0_min,xx0_max,delta), np.arange(xx1_min,xx1_max,delta))

#한번에 flatten 시키는게 있어. revel이다
h = svm_clf.decision_function(np.c_[xx0.ravel(), xx1.ravel()])
#위 함수는 1차원으로만 가능한 함수니까 바꾸고
#다시 reshape한다
h = h.reshape(xx0.shape)

# ax1.clabel(ax1.contour(xx0,xx1, h))#label(등고선 그리기)

# linear 분리에 대해 이렇게 되고
# contour 사이트 들가보면 colormap을 마음대로 쓸 수 있다.

#추가하여 다시 작성하면

ax1.clabel(ax1.contour(xx0,xx1, h, cmap=plt.cm.twilight))#label(등고선 그리기)
#얘가 정확도가 얼마나 되는지 알아보려면 test data를 넣어봐야 함

#  test
[X_train, X_test, Y_train, Y_test] = train_test_split(X_std,Y, test_size = 0.4, random_state = 10, shuffle =True)
# 해주고 svm_clf 부분을 바꾼다
#svm_clf.fit(X_train,Y_train)

print(svm_clf.score(X_test,Y_test))

# 우리가 0번부터 100번까지 했다. 1번하고 2번을 정리했는데
#이걸 만약에 x와 y르,ㄹ 2번하고 3번정리로 할 수 있거든요? 1번하고 2번은 차이가 많이나서 확 벌어진다. 근데 2번3번은 다르다
# 얘네 2,3번으로 해서 score가 얼마나 나오는지 해봐라.
#돌린거를 score와 함께 과제에 올려라.
```

# Linear svm practice iris

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

plt.close("all")

iris = datasets.load_iris()

X = iris["data"][50:150,(2,3)]
Y = iris["target"][50:150]

scaler = StandardScaler()
scaler.fit(X)
X_std = scaler.transform(X)

[X_train, X_test, Y_train, Y_test] = train_test_split(X_std,Y, test_size = 0.4, random_state = 10, shuffle =True)
# 이거는 40퍼센트는 테스트로 두고 나머지 60%를 트레이닝으로 두는 거 의미

f1 = plt.figure(1)
ax1 = f1.add_subplot(111)

df_clf = pd.DataFrame(np.c_[X_std,Y])
df_clf.columns = ["petalLength","petalWidth","target"]
df_clf_group = df_clf.groupby("target")

for target, group in df_clf_group:
    ax1.plot(group.petalLength, group.petalWidth, '*', label="target")

svm_clf = SVC(C=0.01,kernel="linear")

svm_clf.fit(X_train,Y_train)

delta=0.01
[xx0_min, xx0_max] = [min(X_std[:,0])-10*delta, max(X_std[:,0]+10*delta)]
[xx1_min, xx1_max] = [min(X_std[:,1])-10*delta, max(X_std[:,1]+10*delta)]
[xx0, xx1] = np.meshgrid(np.arange(xx0_min,xx0_max,delta), np.arange(xx1_min,xx1_max,delta))

h = svm_clf.decision_function(np.c_[xx0.ravel(), xx1.ravel()])
h = h.reshape(xx0.shape)

ax1.clabel(ax1.contour(xx0,xx1, h, cmap=plt.cm.twilight))

print(svm_clf.score(X_test,Y_test))

# 실행결과 1.0이 출력됩니다.
```
